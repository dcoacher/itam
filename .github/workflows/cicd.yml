# CICD Process (contains multiple phases)
name: CI/CD Pipeline

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
  workflow_dispatch:

env:
  DOCKER_IMAGE: itam-app
  DOCKER_TAG: ${{ github.sha }}

jobs:
  # Phase 1: Test
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flask==3.0.0 pytest==8.2.1

      - name: Run tests
        run: |
          pytest .github/workflows/tests/ -v --tb=short

  # Phase 2: Build Docker Image
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name != 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}
            ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:latest
          cache-from: type=registry,ref=${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:buildcache
          cache-to: type=registry,ref=${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:buildcache,mode=max

  # Phase 3: Infrastructure Setup (Terraform + K8s Cluster)
  infrastructure:
    name: Setup Infrastructure
    runs-on: ubuntu-latest
    needs: build
    if: github.event_name != 'pull_request' && github.ref == 'refs/heads/main'
    outputs:
      control_plane_ip: ${{ steps.terraform-output.outputs.control_plane_ip }}
      worker1_ip: ${{ steps.terraform-output.outputs.worker1_ip }}
      worker2_ip: ${{ steps.terraform-output.outputs.worker2_ip }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.6.0"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Create terraform.tfvars from secrets
        working-directory: iac/terraform
        run: |
          cat > terraform.tfvars <<EOF
          aws_region            = "${{ secrets.AWS_REGION || 'us-east-1' }}"
          aws_access_key_id     = "${{ secrets.AWS_ACCESS_KEY_ID }}"
          aws_secret_access_key = "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          aws_session_token     = "${{ secrets.AWS_SESSION_TOKEN }}"
          ami_id                = "${{ secrets.AWS_AMI_ID || 'ami-0c398cb65a93047f2' }}"
          instance_type         = "${{ secrets.AWS_INSTANCE_TYPE || 't3.medium' }}"
          docker_repo           = "${{ secrets.DOCKER_USERNAME }}"
          EOF

      - name: Terraform Init
        working-directory: iac/terraform
        run: terraform init

      - name: Terraform Plan
        id: terraform-plan
        working-directory: iac/terraform
        run: |
          set +e
          terraform plan -out=tfplan -detailed-exitcode
          PLAN_EXIT_CODE=$?
          set -e
          if [ $PLAN_EXIT_CODE -eq 0 ]; then
            echo "No changes needed. Infrastructure is up to date."
            echo "infrastructure_exists=true" >> $GITHUB_OUTPUT
            echo "has_changes=false" >> $GITHUB_OUTPUT
          elif [ $PLAN_EXIT_CODE -eq 1 ]; then
            echo "ERROR: Terraform plan failed"
            exit 1
          elif [ $PLAN_EXIT_CODE -eq 2 ]; then
            echo "Changes detected. Infrastructure needs to be created or updated."
            echo "infrastructure_exists=false" >> $GITHUB_OUTPUT
            echo "has_changes=true" >> $GITHUB_OUTPUT
          else
            echo "ERROR: Unexpected exit code: $PLAN_EXIT_CODE"
            exit 1
          fi

      - name: Check if outputs exist in state
        id: check-outputs
        working-directory: iac/terraform
        run: |
          # Check if public IP outputs exist in Terraform state
          # Public IPs might be null if instances don't have public IPs assigned
          CONTROL_PLANE_OUTPUT=$(terraform output -raw CONTROL-PLANE-PUBLIC-IP 2>&1 || echo "")
          if [ -n "$CONTROL_PLANE_OUTPUT" ] && [ "$CONTROL_PLANE_OUTPUT" != "null" ] && \
             echo "$CONTROL_PLANE_OUTPUT" | grep -qE "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$"; then
            echo "Public IP outputs exist in Terraform state"
            echo "outputs_exist=true" >> $GITHUB_OUTPUT
          else
            echo "Public IP outputs not found or are null in Terraform state"
            echo "This may mean instances don't have public IPs assigned"
            echo "outputs_exist=false" >> $GITHUB_OUTPUT
          fi

      - name: Import existing resources into Terraform state (if needed)
        working-directory: iac/terraform
        continue-on-error: true
        run: |
          echo "Checking for existing resources that need to be imported..."
          
          # Check if security groups exist and import if needed
          ALB_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=itam-alb-sg" "Name=vpc-id,Values=$(terraform output -raw VPC_ID 2>/dev/null || aws ec2 describe-vpcs --filters 'Name=tag:Name,Values=itam-vpc' --query 'Vpcs[0].VpcId' --output text --region us-east-1 2>/dev/null || echo '')" --region us-east-1 --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null || echo "")
          if [ -n "$ALB_SG_ID" ] && [ "$ALB_SG_ID" != "None" ] && ! terraform state show aws_security_group.ITAM-ALB-SG &>/dev/null 2>&1; then
            echo "Importing existing ALB security group into Terraform state..."
            terraform import aws_security_group.ITAM-ALB-SG "$ALB_SG_ID" || echo "ALB security group import failed or already in state"
          fi
          
          EC2_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=itam-ec2-sg" "Name=vpc-id,Values=$(terraform output -raw VPC_ID 2>/dev/null || aws ec2 describe-vpcs --filters 'Name=tag:Name,Values=itam-vpc' --query 'Vpcs[0].VpcId' --output text --region us-east-1 2>/dev/null || echo '')" --region us-east-1 --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null || echo "")
          if [ -n "$EC2_SG_ID" ] && [ "$EC2_SG_ID" != "None" ] && ! terraform state show aws_security_group.ITAM-EC2-SG &>/dev/null 2>&1; then
            echo "Importing existing EC2 security group into Terraform state..."
            terraform import aws_security_group.ITAM-EC2-SG "$EC2_SG_ID" || echo "EC2 security group import failed or already in state"
          fi
          
          # Check if ALB exists in AWS but not in state
          ALB_ARN=$(aws elbv2 describe-load-balancers --names itam-alb --region us-east-1 --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
          if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ] && ! terraform state show aws_lb.ITAM-ALB &>/dev/null 2>&1; then
            echo "Importing existing ALB into Terraform state..."
            terraform import aws_lb.ITAM-ALB "$ALB_ARN" || echo "ALB import failed or already in state"
          elif [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
            # ALB exists in state, check if security group is correct
            echo "ALB exists in state. Verifying security group configuration..."
            CURRENT_SG=$(aws elbv2 describe-load-balancers --load-balancer-arns "$ALB_ARN" --region us-east-1 --query 'LoadBalancers[0].SecurityGroups[0]' --output text 2>/dev/null || echo "")
            EXPECTED_SG=$(terraform state show aws_security_group.ITAM-ALB-SG 2>/dev/null | grep "^\s*id\s*=" | head -1 | awk -F'"' '{print $2}' || echo "")
            if [ -n "$CURRENT_SG" ] && [ -n "$EXPECTED_SG" ] && [ "$CURRENT_SG" != "$EXPECTED_SG" ]; then
              echo "WARNING: ALB has different security group ($CURRENT_SG) than expected ($EXPECTED_SG)"
              echo "Terraform will attempt to update it. If this fails, manually update the ALB security group in AWS Console."
            fi
          fi
          
          # Check if Target Group exists in AWS but not in state
          TG_ARN=$(aws elbv2 describe-target-groups --names itam-tg --region us-east-1 --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ] && ! terraform state show aws_lb_target_group.ITAM-TG &>/dev/null 2>&1; then
            echo "Importing existing Target Group into Terraform state..."
            terraform import aws_lb_target_group.ITAM-TG "$TG_ARN" || echo "Target Group import failed or already in state"
          fi
          
          # Check if Key Pair exists in AWS but not in state
          # Note: Key pairs with different public keys cannot be imported
          # If key pair exists, we'll need to either delete it or use terraform taint to force recreation
          if aws ec2 describe-key-pairs --key-names itam-keypair --region us-east-1 &>/dev/null 2>&1 && ! terraform state show aws_key_pair.ITAM-KP &>/dev/null 2>&1; then
            echo "WARNING: Key pair 'itam-keypair' exists in AWS but not in Terraform state."
            echo "The key pair will be recreated with a new key. If this causes issues:"
            echo "1. Delete the existing key pair from AWS Console: aws ec2 delete-key-pair --key-name itam-keypair --region us-east-1"
            echo "2. Or manually import it if the public key matches"
            echo "Attempting to delete existing key pair to allow recreation..."
            aws ec2 delete-key-pair --key-name itam-keypair --region us-east-1 2>/dev/null || echo "Could not delete key pair (may need manual deletion)"
          fi

      - name: Handle stuck resources (if any)
        working-directory: iac/terraform
        continue-on-error: true
        run: |
          echo "Checking for stuck resources that might block Terraform apply..."
          
          # Check if target group exists and is stuck
          TG_ARN=$(aws elbv2 describe-target-groups --names itam-tg --region us-east-1 --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ -n "$TG_ARN" ] && [ "$TG_ARN" != "None" ]; then
            echo "Target group found: $TG_ARN"
            
            # Get ALB ARN
            ALB_ARN=$(aws elbv2 describe-load-balancers --names itam-alb --region us-east-1 --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
            
            if [ -n "$ALB_ARN" ] && [ "$ALB_ARN" != "None" ]; then
              # Get listener ARN and check if target group is attached
              LISTENER_ARN=$(aws elbv2 describe-listeners --load-balancer-arn "$ALB_ARN" --region us-east-1 --query 'Listeners[0].ListenerArn' --output text 2>/dev/null || echo "")
              
              if [ -n "$LISTENER_ARN" ] && [ "$LISTENER_ARN" != "None" ]; then
                # Check if listener uses this target group
                LISTENER_TG=$(aws elbv2 describe-listeners --listener-arns "$LISTENER_ARN" --region us-east-1 --query 'Listeners[0].DefaultActions[0].TargetGroupArn' --output text 2>/dev/null || echo "")
                
                if [ "$LISTENER_TG" == "$TG_ARN" ]; then
                  echo "Target group is attached to listener. Removing attachment to allow deletion..."
                  # Delete the listener (it will be recreated by Terraform)
                  aws elbv2 delete-listener --listener-arn "$LISTENER_ARN" --region us-east-1 2>/dev/null || echo "Could not delete listener (may not be necessary)"
                fi
              fi
              
              # Deregister all targets from target group
              echo "Deregistering all targets from target group..."
              TARGETS=$(aws elbv2 describe-target-health --target-group-arn "$TG_ARN" --region us-east-1 --query 'TargetHealthDescriptions[*].Target.Id' --output text 2>/dev/null || echo "")
              if [ -n "$TARGETS" ] && [ "$TARGETS" != "None" ]; then
                for TARGET in $TARGETS; do
                  if [ -n "$TARGET" ] && [ "$TARGET" != "None" ]; then
                    echo "Deregistering target: $TARGET"
                    aws elbv2 deregister-targets --target-group-arn "$TG_ARN" --targets Id="$TARGET" --region us-east-1 2>/dev/null || echo "Could not deregister target $TARGET"
                  fi
                done
                # Wait a moment for deregistration to complete
                sleep 5
              fi
              
              # Now try to delete the target group if it's not in Terraform state
              if ! terraform state show aws_lb_target_group.ITAM-TG &>/dev/null 2>&1; then
                echo "Target group not in Terraform state. Attempting to delete..."
                aws elbv2 delete-target-group --target-group-arn "$TG_ARN" --region us-east-1 2>/dev/null || echo "Could not delete target group (may need to wait or handle manually)"
              fi
            fi
          fi

      - name: Wait for control plane to be SSH-ready (if creating new instances)
        working-directory: iac/terraform
        continue-on-error: true
        run: |
          # Only wait if we're creating new instances
          if [ "${{ steps.terraform-plan.outputs.has_changes }}" == "true" ]; then
            echo "Waiting for control plane instance to be SSH-ready..."
            # Get control plane IP from state (if instance already exists)
            CONTROL_PLANE_IP=$(terraform output -raw CONTROL-PLANE-PUBLIC-IP 2>/dev/null | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" || echo "")
            
            # If not in state, check AWS directly by instance name
            if [ -z "$CONTROL_PLANE_IP" ] || [ "$CONTROL_PLANE_IP" == "null" ]; then
              echo "Control plane IP not in state yet, checking AWS for running instance..."
              CONTROL_PLANE_IP=$(aws ec2 describe-instances \
                --filters "Name=tag:Name,Values=itam-control-plane" "Name=instance-state-name,Values=running" \
                --query 'Reservations[0].Instances[0].PublicIpAddress' \
                --output text --region us-east-1 2>/dev/null | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" || echo "")
            fi
            
            if [ -n "$CONTROL_PLANE_IP" ] && [ "$CONTROL_PLANE_IP" != "null" ] && [ "$CONTROL_PLANE_IP" != "None" ]; then
              echo "Waiting for SSH access to control plane at $CONTROL_PLANE_IP..."
              SSH_KEY_PATH="KP.pem"
              if [ ! -f "$SSH_KEY_PATH" ]; then
                echo "SSH key not found at $SSH_KEY_PATH, skipping SSH readiness check"
                exit 0
              fi
              
              for i in {1..30}; do
                if ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -i "$SSH_KEY_PATH" ubuntu@$CONTROL_PLANE_IP "echo 'SSH ready'" 2>/dev/null; then
                  echo "✓ Control plane is SSH-ready"
                  break
                fi
                if [ $i -eq 30 ]; then
                  echo "⚠ Control plane not SSH-ready after 5 minutes, but continuing..."
                  echo "Terraform will retry with its own timeout settings"
                else
                  echo "Waiting for SSH... ($i/30 attempts)"
                  sleep 10
                fi
              done
            else
              echo "Control plane IP not available yet, Terraform will wait for instance to be ready"
            fi
          fi

      - name: Terraform Apply
        working-directory: iac/terraform
        timeout-minutes: 30
        run: |
          if [ "${{ steps.terraform-plan.outputs.has_changes }}" == "true" ]; then
            echo "Applying Terraform changes..."
            # Set timeout for apply operation (30 minutes total, but individual resource operations have their own timeouts)
            # If apply fails due to existing resources, try to import and retry
            if ! timeout 1800 terraform apply -auto-approve tfplan 2>&1 | tee /tmp/terraform-apply.log; then
              APPLY_EXIT_CODE=${PIPESTATUS[0]}
              if [ $APPLY_EXIT_CODE -eq 124 ]; then
                echo "ERROR: Terraform apply timed out after 30 minutes"
                echo "This usually means a resource is stuck (e.g., target group being destroyed)"
                echo "Check AWS Console for stuck resources and manually resolve if needed"
                exit 1
              elif grep -q "already exists" /tmp/terraform-apply.log; then
                echo "Resources already exist. Attempting to import and retry..."
                terraform refresh
                terraform apply -auto-approve tfplan
              elif grep -q "security groups are invalid\|InvalidSecurityGroup" /tmp/terraform-apply.log; then
                echo "ERROR: Security groups are invalid. Attempting to fix..."
                # Get VPC ID
                VPC_ID=$(terraform state show aws_vpc.ITAM-VPC 2>/dev/null | grep "^\s*id\s*=" | head -1 | awk -F'"' '{print $2}' || echo "")
                if [ -z "$VPC_ID" ]; then
                  VPC_ID=$(aws ec2 describe-vpcs --filters 'Name=tag:Name,Values=itam-vpc' --query 'Vpcs[0].VpcId' --output text --region us-east-1 2>/dev/null || echo "")
                fi
                
                if [ -n "$VPC_ID" ] && [ "$VPC_ID" != "None" ]; then
                  # Check if security groups exist in the correct VPC
                  ALB_SG_ID=$(aws ec2 describe-security-groups --filters "Name=group-name,Values=itam-alb-sg" "Name=vpc-id,Values=$VPC_ID" --region us-east-1 --query 'SecurityGroups[0].GroupId' --output text 2>/dev/null || echo "")
                  
                  if [ -n "$ALB_SG_ID" ] && [ "$ALB_SG_ID" != "None" ]; then
                    echo "Found ALB security group: $ALB_SG_ID"
                    # Import if not in state
                    if ! terraform state show aws_security_group.ITAM-ALB-SG &>/dev/null 2>&1; then
                      echo "Importing ALB security group into state..."
                      terraform import aws_security_group.ITAM-ALB-SG "$ALB_SG_ID" || echo "Import failed"
                    fi
                    # Refresh and retry
                    terraform refresh
                    terraform apply -auto-approve tfplan
                  else
                    echo "ERROR: ALB security group 'itam-alb-sg' not found in VPC $VPC_ID"
                    echo "Please create the security group manually or check Terraform configuration"
                    exit 1
                  fi
                else
                  echo "ERROR: Could not determine VPC ID"
                  exit 1
                fi
              else
                exit 1
              fi
            fi
          elif [ "${{ steps.check-outputs.outputs.outputs_exist }}" != "true" ]; then
            echo "No changes detected, but public IP outputs are missing or null."
            echo "Running terraform apply to refresh state and populate outputs..."
            terraform apply -auto-approve -refresh=true
          else
            echo "No changes detected. Skipping Terraform apply."
            echo "Using existing infrastructure."
            echo "Refreshing Terraform state to ensure outputs are current..."
            terraform refresh || echo "Refresh completed (some resources may not support refresh)"
          fi

      - name: Get Terraform Outputs
        id: terraform-output
        working-directory: iac/terraform
        run: |
          # Check if outputs exist, if not wait a moment and retry
          MAX_RETRIES=5
          RETRY_COUNT=0
          
          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            # Get outputs using -raw, capture both stdout and stderr, filter out warnings
            CONTROL_PLANE_IP=$(terraform output -raw CONTROL-PLANE-PUBLIC-IP 2>&1 | grep -v "Warning:" | grep -v "│" | grep -v "Error:" | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" | head -1 || echo "")
            WORKER1_IP=$(terraform output -raw WORKER-1-PUBLIC-IP 2>&1 | grep -v "Warning:" | grep -v "│" | grep -v "Error:" | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" | head -1 || echo "")
            WORKER2_IP=$(terraform output -raw WORKER-2-PUBLIC-IP 2>&1 | grep -v "Warning:" | grep -v "│" | grep -v "Error:" | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" | head -1 || echo "")
            
            # If Terraform outputs are null or empty, try to get public IPs via AWS CLI
            if [ -z "$CONTROL_PLANE_IP" ] || [ "$CONTROL_PLANE_IP" == "null" ]; then
              echo "Public IP not found in Terraform output for control plane, trying AWS CLI..."
              # Get instance ID from state - try multiple parsing methods
              CONTROL_PLANE_INSTANCE_ID=$(terraform state show aws_instance.CONTROL-PLANE 2>/dev/null | grep -E "^\s*id\s*=" | head -1 | sed 's/.*= "\(.*\)".*/\1/' || echo "")
              if [ -z "$CONTROL_PLANE_INSTANCE_ID" ]; then
                # Alternative: use terraform state list and filter
                CONTROL_PLANE_INSTANCE_ID=$(terraform state list | grep "aws_instance.CONTROL-PLANE" && terraform state show aws_instance.CONTROL-PLANE 2>/dev/null | grep -i "id" | grep -v "arn" | head -1 | awk -F'"' '{print $2}' || echo "")
              fi
              if [ -n "$CONTROL_PLANE_INSTANCE_ID" ] && [ "$CONTROL_PLANE_INSTANCE_ID" != "null" ] && [[ "$CONTROL_PLANE_INSTANCE_ID" =~ ^i- ]]; then
                echo "Found instance ID: $CONTROL_PLANE_INSTANCE_ID"
                CONTROL_PLANE_IP=$(aws ec2 describe-instances --instance-ids "$CONTROL_PLANE_INSTANCE_ID" --query 'Reservations[0].Instances[0].PublicIpAddress' --output text --region us-east-1 2>/dev/null | grep -v "None" | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" || echo "")
                if [ -n "$CONTROL_PLANE_IP" ]; then
                  echo "Found control plane public IP via AWS CLI: $CONTROL_PLANE_IP"
                else
                  echo "Instance $CONTROL_PLANE_INSTANCE_ID does not have a public IP assigned"
                fi
              else
                echo "Could not extract instance ID from Terraform state"
              fi
            fi
            
            if [ -z "$WORKER1_IP" ] || [ "$WORKER1_IP" == "null" ]; then
              echo "Public IP not found in Terraform output for worker 1, trying AWS CLI..."
              WORKER1_INSTANCE_ID=$(terraform state show aws_instance.WORKER-1 2>/dev/null | grep -E "^\s*id\s*=" | head -1 | sed 's/.*= "\(.*\)".*/\1/' || echo "")
              if [ -z "$WORKER1_INSTANCE_ID" ]; then
                WORKER1_INSTANCE_ID=$(terraform state list | grep "aws_instance.WORKER-1" && terraform state show aws_instance.WORKER-1 2>/dev/null | grep -i "id" | grep -v "arn" | head -1 | awk -F'"' '{print $2}' || echo "")
              fi
              if [ -n "$WORKER1_INSTANCE_ID" ] && [ "$WORKER1_INSTANCE_ID" != "null" ] && [[ "$WORKER1_INSTANCE_ID" =~ ^i- ]]; then
                WORKER1_IP=$(aws ec2 describe-instances --instance-ids "$WORKER1_INSTANCE_ID" --query 'Reservations[0].Instances[0].PublicIpAddress' --output text --region us-east-1 2>/dev/null | grep -v "None" | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" || echo "")
                if [ -n "$WORKER1_IP" ]; then
                  echo "Found worker 1 public IP via AWS CLI: $WORKER1_IP"
                fi
              fi
            fi
            
            if [ -z "$WORKER2_IP" ] || [ "$WORKER2_IP" == "null" ]; then
              echo "Public IP not found in Terraform output for worker 2, trying AWS CLI..."
              WORKER2_INSTANCE_ID=$(terraform state show aws_instance.WORKER-2 2>/dev/null | grep -E "^\s*id\s*=" | head -1 | sed 's/.*= "\(.*\)".*/\1/' || echo "")
              if [ -z "$WORKER2_INSTANCE_ID" ]; then
                WORKER2_INSTANCE_ID=$(terraform state list | grep "aws_instance.WORKER-2" && terraform state show aws_instance.WORKER-2 2>/dev/null | grep -i "id" | grep -v "arn" | head -1 | awk -F'"' '{print $2}' || echo "")
              fi
              if [ -n "$WORKER2_INSTANCE_ID" ] && [ "$WORKER2_INSTANCE_ID" != "null" ] && [[ "$WORKER2_INSTANCE_ID" =~ ^i- ]]; then
                WORKER2_IP=$(aws ec2 describe-instances --instance-ids "$WORKER2_INSTANCE_ID" --query 'Reservations[0].Instances[0].PublicIpAddress' --output text --region us-east-1 2>/dev/null | grep -v "None" | grep -E "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" || echo "")
                if [ -n "$WORKER2_IP" ]; then
                  echo "Found worker 2 public IP via AWS CLI: $WORKER2_IP"
                fi
              fi
            fi
            
            # Check if we got valid IPs (not empty and matches IP pattern)
            if [ -n "$CONTROL_PLANE_IP" ] && [ -n "$WORKER1_IP" ] && [ -n "$WORKER2_IP" ] && \
               echo "$CONTROL_PLANE_IP" | grep -qE "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" && \
               echo "$WORKER1_IP" | grep -qE "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$" && \
               echo "$WORKER2_IP" | grep -qE "^[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+$"; then
              echo "✓ Successfully retrieved Terraform outputs"
              echo "Control Plane IP: $CONTROL_PLANE_IP"
              echo "Worker 1 IP: $WORKER1_IP"
              echo "Worker 2 IP: $WORKER2_IP"
              echo "control_plane_ip=$CONTROL_PLANE_IP" >> $GITHUB_OUTPUT
              echo "worker1_ip=$WORKER1_IP" >> $GITHUB_OUTPUT
              echo "worker2_ip=$WORKER2_IP" >> $GITHUB_OUTPUT
              break
            else
              RETRY_COUNT=$((RETRY_COUNT + 1))
              if [ $RETRY_COUNT -lt $MAX_RETRIES ]; then
                echo "Outputs not ready yet. Retrying in 5 seconds... ($RETRY_COUNT/$MAX_RETRIES)"
                echo "Debug - Control Plane: '$CONTROL_PLANE_IP'"
                echo "Debug - Worker 1: '$WORKER1_IP'"
                echo "Debug - Worker 2: '$WORKER2_IP'"
                sleep 5
              else
                echo "ERROR: Failed to retrieve Terraform outputs after $MAX_RETRIES attempts"
                echo ""
                echo "=== Diagnostic Information ==="
                echo "Checking Terraform state..."
                terraform state list 2>&1 | head -20 || echo "No state found"
                echo ""
                echo "Checking if resources exist in state..."
                terraform state show aws_instance.CONTROL-PLANE 2>&1 | head -5 || echo "Control plane not in state"
                echo ""
                echo "Checking all Terraform outputs..."
                terraform output 2>&1 || echo "No outputs available"
                echo ""
                echo "If outputs are missing, this usually means:"
                echo "1. Infrastructure was created but outputs weren't saved to state"
                echo "2. State file is missing or corrupted"
                echo "3. Outputs need to be populated by running 'terraform apply'"
                exit 1
              fi
            fi
          done

      - name: Upload SSH key as artifact
        uses: actions/upload-artifact@v4
        with:
          name: ssh-key
          path: iac/terraform/KP.pem
          retention-days: 1

      - name: Wait for EC2 instances to be ready
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          WORKER1_IP="${{ steps.terraform-output.outputs.worker1_ip }}"
          WORKER2_IP="${{ steps.terraform-output.outputs.worker2_ip }}"
          echo "Waiting for instances to be ready..."
          for IP in "$CONTROL_PLANE_IP" "$WORKER1_IP" "$WORKER2_IP"; do
            echo "Waiting for $IP..."
            for i in {1..30}; do
              if ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -i iac/terraform/KP.pem ubuntu@$IP "echo 'ready'" 2>/dev/null; then
                echo "$IP is ready"
                break
              fi
              echo "Attempt $i/30..."
              sleep 10
            done
          done
          
          # Wait a bit more for user-data scripts to start
          echo "Waiting for user-data scripts to initialize..."
          sleep 30

      - name: Check user-data script progress on control plane
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          echo "Checking user-data script status..."
          # Check if cloud-init is still running
          CLOUD_INIT_STATUS=$(ssh -i iac/terraform/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo cloud-init status 2>/dev/null || echo 'unknown'" 2>/dev/null || echo "unknown")
          echo "Cloud-init status: $CLOUD_INIT_STATUS"
          # Check if kubeadm is installed (indicates user-data is progressing)
          KUBEADM_INSTALLED=$(ssh -i iac/terraform/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "command -v kubeadm >/dev/null 2>&1 && echo 'yes' || echo 'no'" 2>/dev/null || echo "no")
          echo "kubeadm installed: $KUBEADM_INSTALLED"
          # Show recent cloud-init logs
          echo "Recent cloud-init logs:"
          ssh -i iac/terraform/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo tail -20 /var/log/cloud-init-output.log" 2>/dev/null || echo "Could not retrieve logs"

      - name: Set up SSH keys
        run: |
          mkdir -p ~/.ssh
          cp iac/terraform/KP.pem ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          WORKER1_IP="${{ steps.terraform-output.outputs.worker1_ip }}"
          WORKER2_IP="${{ steps.terraform-output.outputs.worker2_ip }}"
          ssh-keyscan -H $CONTROL_PLANE_IP >> ~/.ssh/known_hosts 2>/dev/null || true
          ssh-keyscan -H $WORKER1_IP >> ~/.ssh/known_hosts 2>/dev/null || true
          ssh-keyscan -H $WORKER2_IP >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Wait for Kubernetes control plane to be ready
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          echo "Waiting for Kubernetes API server to be ready..."
          echo "This may take 3-5 minutes after instance startup..."
          
          # First, wait for kubeadm init to complete
          echo "Step 1: Waiting for kubeadm init to complete..."
          for i in {1..60}; do
            if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "test -f /etc/kubernetes/admin.conf" 2>/dev/null; then
              echo "✓ kubeadm init completed (admin.conf exists)"
              break
            fi
            if [ $i -eq 60 ]; then
              echo "ERROR: kubeadm init did not complete after 10 minutes"
              echo "Checking kubeadm status..."
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo journalctl -u kubelet --no-pager -n 50" || true
              exit 1
            fi
            echo "Waiting for kubeadm init... ($i/60 attempts)"
            sleep 10
          done
          
          # Wait for API server pod to be running
          echo "Step 2: Waiting for API server pod to be running..."
          for i in {1..60}; do
            API_STATUS=$(ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo crictl ps --name kube-apiserver --state Running 2>/dev/null | grep -c kube-apiserver || echo '0'" 2>/dev/null || echo "0")
            if [ "$API_STATUS" != "0" ]; then
              echo "✓ API server pod is running"
              break
            fi
            if [ $i -eq 60 ]; then
              echo "ERROR: API server pod not running after 10 minutes"
              echo "Checking pod status..."
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo crictl ps -a | grep kube-apiserver" || true
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo journalctl -u kubelet --no-pager -n 50" || true
              exit 1
            fi
            echo "Waiting for API server pod... ($i/60 attempts)"
            sleep 10
          done
          
          # Wait for API server to respond
          echo "Step 3: Waiting for API server to respond..."
          for i in {1..60}; do
            if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config cluster-info &>/dev/null" 2>/dev/null; then
              echo "✓ Kubernetes API server is ready and responding"
              # Show cluster info
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config cluster-info" || true
              break
            fi
            if [ $i -eq 60 ]; then
              echo "ERROR: API server not responding after 10 minutes"
              echo "Checking API server logs..."
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo crictl logs \$(sudo crictl ps --name kube-apiserver -q | head -1) --tail 50" 2>/dev/null || true
              echo "Checking kubelet status..."
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo systemctl status kubelet --no-pager -l" || true
              exit 1
            fi
            echo "Waiting for API server response... ($i/60 attempts)"
            sleep 10
          done
          
          echo "✓ Kubernetes control plane is fully ready"

      - name: Get join command from control plane
        id: join-command
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          JOIN_CMD=$(ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "cat /home/ubuntu/join-command.sh")
          echo "join_command<<EOF" >> $GITHUB_OUTPUT
          echo "$JOIN_CMD" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Rename and join worker 1
        run: |
          WORKER1_IP="${{ steps.terraform-output.outputs.worker1_ip }}"
          JOIN_CMD="${{ steps.join-command.outputs.join_command }}"
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$WORKER1_IP <<ENDSSH
            set -e
            INSTANCE_ID=\$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
            sudo hostnamectl set-hostname k8s-worker-\${INSTANCE_ID}
            echo "Worker 1 hostname set to: k8s-worker-\${INSTANCE_ID}"
            sudo $JOIN_CMD
          ENDSSH

      - name: Rename and join worker 2
        run: |
          WORKER2_IP="${{ steps.terraform-output.outputs.worker2_ip }}"
          JOIN_CMD="${{ steps.join-command.outputs.join_command }}"
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$WORKER2_IP <<ENDSSH
            set -e
            INSTANCE_ID=\$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
            sudo hostnamectl set-hostname k8s-worker-\${INSTANCE_ID}
            echo "Worker 2 hostname set to: k8s-worker-\${INSTANCE_ID}"
            sudo $JOIN_CMD
          ENDSSH

      - name: Wait for all nodes to be ready
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          echo "Waiting for all nodes to join the cluster..."
          for i in {1..30}; do
            NODE_COUNT=$(ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config get nodes --no-headers 2>/dev/null | wc -l" || echo "0")
            READY_COUNT=$(ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config get nodes --no-headers 2>/dev/null | grep -c Ready || echo '0'")
            echo "Nodes: $READY_COUNT/$NODE_COUNT ready (attempt $i/30)"
            if [ "$NODE_COUNT" -ge "3" ] && [ "$READY_COUNT" -ge "3" ]; then
              echo "All nodes are ready!"
              break
            fi
            sleep 10
          done
          # Show node status
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config get nodes"


  # Phase 4: Deploy to Kubernetes
  deploy:
    name: Deploy to Kubernetes
    runs-on: ubuntu-latest
    needs: [build, infrastructure]
    if: github.event_name != 'pull_request' && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Download SSH key artifact
        uses: actions/download-artifact@v4
        with:
          name: ssh-key
          path: ~/.ssh/

      - name: Set up SSH and get kubeconfig
        run: |
          chmod 600 ~/.ssh/KP.pem
          CONTROL_PLANE_IP="${{ needs.infrastructure.outputs.control_plane_ip }}"
          mkdir -p ~/.kube
          mkdir -p ~/.ssh
          ssh-keyscan -H $CONTROL_PLANE_IP >> ~/.ssh/known_hosts 2>/dev/null || true
          # Get kubeconfig from control plane
          ssh -i ~/.ssh/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "cat /home/ubuntu/.kube/config" > ~/.kube/config
          chmod 600 ~/.kube/config
          # Update to use public IP
          sed -i "s|10.0.1.10:6443|$CONTROL_PLANE_IP:6443|g" ~/.kube/config
          kubectl config set-cluster kubernetes --insecure-skip-tls-verify=true
          # Verify
          kubectl cluster-info
          kubectl get nodes

      - name: Deploy NFS PersistentVolume
        run: |
          kubectl apply -f - <<EOF || true
          apiVersion: v1
          kind: PersistentVolume
          metadata:
            name: itam-nfs-pv
            labels:
              type: nfs
          spec:
            capacity:
              storage: 10Gi
            accessModes:
              - ReadWriteMany
            persistentVolumeReclaimPolicy: Retain
            storageClassName: nfs-client
            nfs:
              path: /srv/nfs/k8s
              server: 10.0.1.10
            mountOptions:
              - soft
              - timeo=30
              - retrans=3
              - nfsvers=4.1
          ---
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: nfs-client
          provisioner: kubernetes.io/no-provisioner
          volumeBindingMode: Immediate
          EOF

      - name: Deploy application using deploy.sh
        run: |
          set -e
          CONTROL_PLANE_IP="${{ needs.infrastructure.outputs.control_plane_ip }}"
          IMAGE_REPO="${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}"
          IMAGE_TAG="${{ env.DOCKER_TAG }}"
          echo "Deploying with image: $IMAGE_REPO:$IMAGE_TAG"
          ssh -i ~/.ssh/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP <<ENDSSH
            cd /home/ubuntu/helm
            ./deploy.sh "$IMAGE_REPO" "$IMAGE_TAG"
          ENDSSH

      - name: Verify deployment
        run: |
          echo "=== Deployment Status ==="
          kubectl get pods -l app=itam-app
          kubectl get svc itam-app
          echo ""
          echo "✓ Deployment completed"
          echo "Image deployed: ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}"