# CICD Process (contains multiple phases)
name: CI/CD Pipeline

on:
  push:
    branches:
      - main
      - develop
  pull_request:
    branches:
      - main
  workflow_dispatch:

env:
  DOCKER_IMAGE: itam-app
  DOCKER_TAG: ${{ github.sha }}

jobs:
  # Phase 1: Test
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install flask==3.0.0 pytest==8.2.1

      - name: Run tests
        run: |
          pytest .github/workflows/tests/ -v --tb=short

  # Phase 2: Build Docker Image
  build:
    name: Build Docker Image
    runs-on: ubuntu-latest
    needs: test
    if: github.event_name != 'pull_request'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Log in to Docker Hub
        if: github.event_name != 'pull_request'
        uses: docker/login-action@v3
        with:
          username: ${{ secrets.DOCKER_USERNAME }}
          password: ${{ secrets.DOCKER_PASSWORD }}

      - name: Build Docker image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./docker/Dockerfile
          push: ${{ github.event_name != 'pull_request' }}
          tags: |
            ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}
            ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:latest
          cache-from: type=registry,ref=${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:buildcache
          cache-to: type=registry,ref=${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:buildcache,mode=max

  # Phase 3: Infrastructure Setup (Terraform + K8s Cluster)
  infrastructure:
    name: Setup Infrastructure
    runs-on: ubuntu-latest
    needs: build
    if: github.event_name != 'pull_request' && github.ref == 'refs/heads/main'
    outputs:
      control_plane_ip: ${{ steps.terraform-output.outputs.control_plane_ip }}
      worker1_ip: ${{ steps.terraform-output.outputs.worker1_ip }}
      worker2_ip: ${{ steps.terraform-output.outputs.worker2_ip }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Setup Terraform
        uses: hashicorp/setup-terraform@v3
        with:
          terraform_version: "1.6.0"

      - name: Configure AWS credentials
        uses: aws-actions/configure-aws-credentials@v4
        with:
          aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
          aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          aws-session-token: ${{ secrets.AWS_SESSION_TOKEN }}
          aws-region: us-east-1

      - name: Create terraform.tfvars from secrets
        working-directory: iac/terraform
        run: |
          cat > terraform.tfvars <<EOF
          aws_region            = "${{ secrets.AWS_REGION || 'us-east-1' }}"
          aws_access_key_id     = "${{ secrets.AWS_ACCESS_KEY_ID }}"
          aws_secret_access_key = "${{ secrets.AWS_SECRET_ACCESS_KEY }}"
          aws_session_token     = "${{ secrets.AWS_SESSION_TOKEN }}"
          ami_id                = "${{ secrets.AWS_AMI_ID || 'ami-0c398cb65a93047f2' }}"
          instance_type         = "${{ secrets.AWS_INSTANCE_TYPE || 't3.medium' }}"
          docker_repo           = "${{ secrets.DOCKER_USERNAME }}"
          EOF

      - name: Terraform Init
        working-directory: iac/terraform
        run: terraform init

      - name: Terraform Plan
        working-directory: iac/terraform
        run: terraform plan -out=tfplan

      - name: Terraform Apply
        working-directory: iac/terraform
        run: terraform apply -auto-approve tfplan

      - name: Get Terraform Outputs
        id: terraform-output
        working-directory: iac/terraform
        run: |
          CONTROL_PLANE_IP=$(terraform output -raw CONTROL-PLANE-PUBLIC-IP)
          WORKER1_IP=$(terraform output -raw WORKER-1-PUBLIC-IP)
          WORKER2_IP=$(terraform output -raw WORKER-2-PUBLIC-IP)
          echo "control_plane_ip=$CONTROL_PLANE_IP" >> $GITHUB_OUTPUT
          echo "worker1_ip=$WORKER1_IP" >> $GITHUB_OUTPUT
          echo "worker2_ip=$WORKER2_IP" >> $GITHUB_OUTPUT

      - name: Upload SSH key as artifact
        uses: actions/upload-artifact@v4
        with:
          name: ssh-key
          path: iac/terraform/KP.pem
          retention-days: 1

      - name: Wait for EC2 instances to be ready
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          WORKER1_IP="${{ steps.terraform-output.outputs.worker1_ip }}"
          WORKER2_IP="${{ steps.terraform-output.outputs.worker2_ip }}"
          echo "Waiting for instances to be ready..."
          for IP in "$CONTROL_PLANE_IP" "$WORKER1_IP" "$WORKER2_IP"; do
            echo "Waiting for $IP..."
            for i in {1..30}; do
              if ssh -o ConnectTimeout=5 -o StrictHostKeyChecking=no -i iac/terraform/KP.pem ubuntu@$IP "echo 'ready'" 2>/dev/null; then
                echo "$IP is ready"
                break
              fi
              echo "Attempt $i/30..."
              sleep 10
            done
          done
          
          # Wait a bit more for user-data scripts to start
          echo "Waiting for user-data scripts to initialize..."
          sleep 30

      - name: Check user-data script progress on control plane
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          echo "Checking user-data script status..."
          # Check if cloud-init is still running
          CLOUD_INIT_STATUS=$(ssh -i iac/terraform/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo cloud-init status 2>/dev/null || echo 'unknown'" 2>/dev/null || echo "unknown")
          echo "Cloud-init status: $CLOUD_INIT_STATUS"
          # Check if kubeadm is installed (indicates user-data is progressing)
          KUBEADM_INSTALLED=$(ssh -i iac/terraform/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "command -v kubeadm >/dev/null 2>&1 && echo 'yes' || echo 'no'" 2>/dev/null || echo "no")
          echo "kubeadm installed: $KUBEADM_INSTALLED"
          # Show recent cloud-init logs
          echo "Recent cloud-init logs:"
          ssh -i iac/terraform/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo tail -20 /var/log/cloud-init-output.log" 2>/dev/null || echo "Could not retrieve logs"

      - name: Set up SSH keys
        run: |
          mkdir -p ~/.ssh
          cp iac/terraform/KP.pem ~/.ssh/id_rsa
          chmod 600 ~/.ssh/id_rsa
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          WORKER1_IP="${{ steps.terraform-output.outputs.worker1_ip }}"
          WORKER2_IP="${{ steps.terraform-output.outputs.worker2_ip }}"
          ssh-keyscan -H $CONTROL_PLANE_IP >> ~/.ssh/known_hosts 2>/dev/null || true
          ssh-keyscan -H $WORKER1_IP >> ~/.ssh/known_hosts 2>/dev/null || true
          ssh-keyscan -H $WORKER2_IP >> ~/.ssh/known_hosts 2>/dev/null || true

      - name: Wait for Kubernetes control plane to be ready
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          echo "Waiting for Kubernetes API server to be ready..."
          echo "This may take 3-5 minutes after instance startup..."
          
          # First, wait for kubeadm init to complete
          echo "Step 1: Waiting for kubeadm init to complete..."
          for i in {1..60}; do
            if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "test -f /etc/kubernetes/admin.conf" 2>/dev/null; then
              echo "✓ kubeadm init completed (admin.conf exists)"
              break
            fi
            if [ $i -eq 60 ]; then
              echo "ERROR: kubeadm init did not complete after 10 minutes"
              echo "Checking kubeadm status..."
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo journalctl -u kubelet --no-pager -n 50" || true
              exit 1
            fi
            echo "Waiting for kubeadm init... ($i/60 attempts)"
            sleep 10
          done
          
          # Wait for API server pod to be running
          echo "Step 2: Waiting for API server pod to be running..."
          for i in {1..60}; do
            API_STATUS=$(ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo crictl ps --name kube-apiserver --state Running 2>/dev/null | grep -c kube-apiserver || echo '0'" 2>/dev/null || echo "0")
            if [ "$API_STATUS" != "0" ]; then
              echo "✓ API server pod is running"
              break
            fi
            if [ $i -eq 60 ]; then
              echo "ERROR: API server pod not running after 10 minutes"
              echo "Checking pod status..."
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo crictl ps -a | grep kube-apiserver" || true
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo journalctl -u kubelet --no-pager -n 50" || true
              exit 1
            fi
            echo "Waiting for API server pod... ($i/60 attempts)"
            sleep 10
          done
          
          # Wait for API server to respond
          echo "Step 3: Waiting for API server to respond..."
          for i in {1..60}; do
            if ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config cluster-info &>/dev/null" 2>/dev/null; then
              echo "✓ Kubernetes API server is ready and responding"
              # Show cluster info
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config cluster-info" || true
              break
            fi
            if [ $i -eq 60 ]; then
              echo "ERROR: API server not responding after 10 minutes"
              echo "Checking API server logs..."
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo crictl logs \$(sudo crictl ps --name kube-apiserver -q | head -1) --tail 50" 2>/dev/null || true
              echo "Checking kubelet status..."
              ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "sudo systemctl status kubelet --no-pager -l" || true
              exit 1
            fi
            echo "Waiting for API server response... ($i/60 attempts)"
            sleep 10
          done
          
          echo "✓ Kubernetes control plane is fully ready"

      - name: Get join command from control plane
        id: join-command
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          JOIN_CMD=$(ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "cat /home/ubuntu/join-command.sh")
          echo "join_command<<EOF" >> $GITHUB_OUTPUT
          echo "$JOIN_CMD" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

      - name: Rename and join worker 1
        run: |
          WORKER1_IP="${{ steps.terraform-output.outputs.worker1_ip }}"
          JOIN_CMD="${{ steps.join-command.outputs.join_command }}"
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$WORKER1_IP <<ENDSSH
            set -e
            INSTANCE_ID=\$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
            sudo hostnamectl set-hostname k8s-worker-\${INSTANCE_ID}
            echo "Worker 1 hostname set to: k8s-worker-\${INSTANCE_ID}"
            sudo $JOIN_CMD
          ENDSSH

      - name: Rename and join worker 2
        run: |
          WORKER2_IP="${{ steps.terraform-output.outputs.worker2_ip }}"
          JOIN_CMD="${{ steps.join-command.outputs.join_command }}"
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$WORKER2_IP <<ENDSSH
            set -e
            INSTANCE_ID=\$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
            sudo hostnamectl set-hostname k8s-worker-\${INSTANCE_ID}
            echo "Worker 2 hostname set to: k8s-worker-\${INSTANCE_ID}"
            sudo $JOIN_CMD
          ENDSSH

      - name: Wait for all nodes to be ready
        run: |
          CONTROL_PLANE_IP="${{ steps.terraform-output.outputs.control_plane_ip }}"
          echo "Waiting for all nodes to join the cluster..."
          for i in {1..30}; do
            NODE_COUNT=$(ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config get nodes --no-headers 2>/dev/null | wc -l" || echo "0")
            READY_COUNT=$(ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config get nodes --no-headers 2>/dev/null | grep -c Ready || echo '0'")
            echo "Nodes: $READY_COUNT/$NODE_COUNT ready (attempt $i/30)"
            if [ "$NODE_COUNT" -ge "3" ] && [ "$READY_COUNT" -ge "3" ]; then
              echo "All nodes are ready!"
              break
            fi
            sleep 10
          done
          # Show node status
          ssh -i ~/.ssh/id_rsa -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "kubectl --kubeconfig=/home/ubuntu/.kube/config get nodes"


  # Phase 4: Deploy to Kubernetes
  deploy:
    name: Deploy to Kubernetes
    runs-on: ubuntu-latest
    needs: [build, infrastructure]
    if: github.event_name != 'pull_request' && github.ref == 'refs/heads/main'
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Configure kubectl
        uses: azure/setup-kubectl@v3
        with:
          version: 'latest'

      - name: Set up Helm
        uses: azure/setup-helm@v3
        with:
          version: 'latest'

      - name: Download SSH key artifact
        uses: actions/download-artifact@v4
        with:
          name: ssh-key
          path: ~/.ssh/

      - name: Set up SSH and get kubeconfig
        run: |
          chmod 600 ~/.ssh/KP.pem
          CONTROL_PLANE_IP="${{ needs.infrastructure.outputs.control_plane_ip }}"
          mkdir -p ~/.kube
          mkdir -p ~/.ssh
          ssh-keyscan -H $CONTROL_PLANE_IP >> ~/.ssh/known_hosts 2>/dev/null || true
          # Get kubeconfig from control plane
          ssh -i ~/.ssh/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP "cat /home/ubuntu/.kube/config" > ~/.kube/config
          chmod 600 ~/.kube/config
          # Update to use public IP
          sed -i "s|10.0.1.10:6443|$CONTROL_PLANE_IP:6443|g" ~/.kube/config
          kubectl config set-cluster kubernetes --insecure-skip-tls-verify=true
          # Verify
          kubectl cluster-info
          kubectl get nodes

      - name: Deploy NFS PersistentVolume
        run: |
          kubectl apply -f - <<EOF || true
          apiVersion: v1
          kind: PersistentVolume
          metadata:
            name: itam-nfs-pv
            labels:
              type: nfs
          spec:
            capacity:
              storage: 10Gi
            accessModes:
              - ReadWriteMany
            persistentVolumeReclaimPolicy: Retain
            storageClassName: nfs-client
            nfs:
              path: /srv/nfs/k8s
              server: 10.0.1.10
            mountOptions:
              - soft
              - timeo=30
              - retrans=3
              - nfsvers=4.1
          ---
          apiVersion: storage.k8s.io/v1
          kind: StorageClass
          metadata:
            name: nfs-client
          provisioner: kubernetes.io/no-provisioner
          volumeBindingMode: Immediate
          EOF

      - name: Deploy application using deploy.sh
        run: |
          set -e
          CONTROL_PLANE_IP="${{ needs.infrastructure.outputs.control_plane_ip }}"
          IMAGE_REPO="${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}"
          IMAGE_TAG="${{ env.DOCKER_TAG }}"
          echo "Deploying with image: $IMAGE_REPO:$IMAGE_TAG"
          ssh -i ~/.ssh/KP.pem -o StrictHostKeyChecking=no ubuntu@$CONTROL_PLANE_IP <<ENDSSH
            cd /home/ubuntu/helm
            ./deploy.sh "$IMAGE_REPO" "$IMAGE_TAG"
          ENDSSH

      - name: Verify deployment
        run: |
          echo "=== Deployment Status ==="
          kubectl get pods -l app=itam-app
          kubectl get svc itam-app
          echo ""
          echo "✓ Deployment completed"
          echo "Image deployed: ${{ secrets.DOCKER_USERNAME }}/${{ env.DOCKER_IMAGE }}:${{ env.DOCKER_TAG }}"